fragment_downloaded_cb({"url":"docs/configuration-ini.html#page-description","fragment":"NNTrainer requires network configuration file which includes network layers and hyper-parameters. The format of configuration file is iniparser format which is commonly used. Keywords are not case sensitive and the line start with '#' will be ignored.\nIf you want more about iniparser, please visit\nhttps://github.com/ndevilla/iniparser\nConfiguration file consists of Two Sections, Network and Layer\nNetwork section includes the hyper-parameters about Network such as batch size, name of model file to save trained weight, epochs and etc.\nStart with \"[Model]\"\ntype (mandatory) = <string>\nType of Network\nlearning_rate = <float>\nInitial learning rate to decay\nepochs = <unsigned int>\nNumber of epochs to train\noptimizer = <string>\nOptimizer to apply the gradients to weights.\nloss = <string>\nLoss function\nsave_path = <string>\nModel file path to save updated weights\nbatch_size = <unsigned int>\nMini batch size\nbeta1 = <float>\nbeta1 parameter for adam optimizer. Only valid for adam.   0.9 is default.\nbeta2 = <float>\nbeta2 parameter for adam optimizer. Only valid for adam. 0.999 is default.\nepsilon = <float>\nEpsilon parameter for adam optimizer. Only valid for adam. 1.0e-7 is default.\nBelow is sample Network section.\nDefine data set. training, validation, test data set.\nStart with \"[ DataSet ]\"\nbuffersize = <unsigned int>\nDefine Buffer size. usually it is greater than batch size.\nData buffer thread keeps reading the data from the file and stores the data into the data buffer.\nMeanwhile main thread gets the training data from this data buffer and feeds it to the model.\nThis keyword defines the size of Data Buffer.\ntraindata = <string>\ntraining data file path.   The data must be saved as following\nfeature data[i], label data[i], feature data[i+1], label data[i+1], ...\nvaliddata = <string>\nvalidation data file path.   The data must be saved as following\nfeature data[i], label data[i], feature data[i+1], label data[i+1], ...\ntestdata = <string>\ntest data file path.   The data must be saved as following\nfeature data[i], label data[i], feature data[i+1], label data[i+1], ...\nlabeldata = <string>\nlabel data file path. The data must be saved as following\nclass Name [i], class name [i+1],...\nDescribe hyper-parameters for layer. Order of layers in the model follows the order of definition of layers here from top to bottom.\nStart with \"[ ${layer name} ]\". This layer name must be unique throughout network model.\ntype = <string>\nType of Layer\nkernel_size = <unsigned int>,<unsigned int>\nKernel size for convolution layer\nbias_init_zero = <bool>\ntoken to initialize bias with zeros. Setting to False would initialize bias randomly.\nnormalization = <bool>\nnormalization on the input of this layer.\nstandardization = <bool>\nstandardization on the input of this layer.\ninput_shape = <unsigned int>:<unsigned int>:<unsigned int>\nshape of input (shouldn't be zero).\nactivation = <string>\nset activation layer\nweight_decay = <string>\nset weight decay\nweight_regularizer_constant = <float>\ncoefficient for weight decay\nunit = <unsigned int>\nset the output layer for fully connected layer\nweight_initializer = <string>\nset weight initialization method\nfilters = <unsigned int>\nset filters size for convolution layer\nstride = <unsigned int>,<unsigned int>\nset stride for convolution and pooling layer\npadding = <unsigned int>,<unsigned int>\nset padding for convolution and pooling layer\npool_size = <unsigned int>,<unsigned int>\nset pooling size for pooling layer\npooling = <string>\ndefine type of pooling\nflatten = <bool>\nflattens the output of this layer.\nEnabling this option is equivalent to attaching a flatten layer after the current layer.\nepsilon = <float>\nEpsilon parameter for batch normalization layer. Default is 0.001.\nEach layer requires different properties.\n| Layer | Properties |\n|:-------:|:---|\n| conv2d |\nBelow is sample for layers to define a model.\nThis allows to describe another model, termed as backbone, to be used in the model described by the current ini file.\nThe backbone to be used can be described with another ini configuration file path, or with model file for external frameworks.\nSupport for backbones of external framework for Tensorflow-Lite is provided natively with Tensorflow-Lite framework.\nSupport for backbones of other external frameworks is done using nnstreamer and its plugin.\nWhen using nnstreamer for external framework, ensure to add the corresponding baseline ML framework and its corresponding nnstreamer plugin as a dependency or install manually.\nFor example, when using PyTorch based model as a backbone, both the packages PyTorch and nnstreamer-pytorch must be installed.\nBackbones made of nntrainer models, described using ini, support training the backbone also.\nHowever, this is not supported with external frameworks.\nIt is possible to describe a backbone inside a backbone ini configuration file, as well as listing down multiple backbones to build a single model.\nFor backbone ini configuration file, Model and Dataset sections are ignored.\nDescribing a backbone is very similar to describing a layer.\nStart with a \"[ ${layer name} ]\" which must be unique throughtout the model. In case of backbone, the name of the backbone is prepended to the name of all the layers inside the backbone.\nbackbone = <string>\nPath of the backbone file. Supported model files:\ntrainable = <bool>\nIf this backbone must be trained (defaults to false). Only supported for ini backbones (nntrainer models).\nPreload = <bool>\nLoad pretrained weights from the saved modelfile of backbone (defaults to false). Only supported for ini backbone (nntrainer models).\nScaleSize = <float>\nScale the size of the layers from backbone (defaults to 1.0). This applies for fully connected and convolution layer for now, where the units and the output channels are scaled respectively. Only supported for ini backbone (nntrainer models). If the model is being scaled, it cannot be preloaded from the saved modelfile. Only of the two options, ScaleSize and Preload, must be set at once.\nInputShape = <string>\nSet the shape of the input layer for the backbone model. Only supported for ini backbones (nntrainer models).\nInputLayer = <string>\nChoose the start layer for the backbone. This allows taking a subgraph starting with the specified layer name as a backbone. Only supported for ini backbones (nntrainer models).\nOutputLayer = <string>\nChoose the end layer for the backbone. This allows taking a subgraph ending with the specified layer name as a backbone. Only supported for ini backbones (nntrainer models).\n``\nBelow is sample backbone section.\nThis has one input layer, two convolution layers, two pooling layers, one flatten layer and one fully connected layer to classify MNIST example.\nIt takes 1 x 28 x 28 gray data (0~255) as an input. Adam optimizer is used to apply gradient and learning rate is 1.0e-4.\n\nregression : network for linear regression\nknn : K-nearest neighbor\nneuralnetwork : Deep Neural Network\n\n\nadam : Adaptive Moment Estimation\nsgd : stochastic gradient decent\n\n\nmse : mean squared error\ncross : cross entropy\nOnly allowed with sigmoid and softmax activation function\n\n\ninput : input layer\nconv2d : 2D convolution layer\npooling2d : 2D pooling layer\nflatten : flatten layer\nfully_connected : fully connected layer\nbatch_normalization : batch normalization layer\nactivation : activation layer\n\n\ntanh : tanh function\nsigmoid : sigmoid function\nrelu : ReLU function\nsoftmax : softmax function\n\n\nl2norm : L2 normalization\n\n\nzeros : Zero initialization\nlecun_normal : LeCun normal initialization\nlecun_uniform : LeCun uniform initialization\nxavier_normal : xavier normal initialization\nxavier_uniform : xavier uniform initialization\nhe_normal : He normal initialization\nhe_uniform : He uniform initialization\n\n\nmax : max pooling\naverage : average pooling\nglobal_max : global max pooling\nglobal_average : global average pooling\n\n\nfilters\nkernel_size\nstride\npadding\nnormalization\nstandardization\ninput_shape\nbias_init_zero\nactivation\nflatten\nweight_decay\nweight_regularizer_constant\nweight_initializer\n\n\npooling\npool_size\nstride\npadding\n\n\n.ini - NNTrainer models\n.tflite - Tensorflow-Lite models\n.pb / .pt / .py / .circle etc via NNStreamer (corresponding nnstreamer plugin required)\n\n"});