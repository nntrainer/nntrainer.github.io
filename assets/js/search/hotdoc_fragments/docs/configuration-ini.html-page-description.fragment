fragment_downloaded_cb({"url": "docs/configuration-ini.html#page-description", "fragment": "NNTrainer requires network configuration file which includes network layers and hyper parameters. The format of configuration file is iniparser format which is commonly used. Keywords are not case sensitive and the line start with will be ignored. \nIf you want more about iniparser please visit https github.com ndevilla iniparser \nConfiguration file consists of Two Sections Network and Layer \nNetwork section includes the hyper parameters about Network such as batch size name of model file to save trained weight epochs and etc. \nStart with Model \ntype mandatory string \nType of Network \nepochs unsigned int \nNumber of epochs to train \nCreate a new section for this \nloss string \nLoss function \nsave_path string \nModel file path to save updated weights \nbatch_size unsigned int \nMini batch size \nBelow is sample Network section. \nDefine the optimizer to be used for training. This is an optional section needed only for training and can be skipped for inference. \nStart with Optimizer \ntype string \nOptimizer type to apply the gradients to weights. \nlearning_rate float \nInitial learning rate to decay \nbeta1 float \nbeta1 parameter for adam optimizer. Only valid for adam. is default. \nbeta2 float \nbeta2 parameter for adam optimizer. Only valid for adam. is default. \nepsilon float \nEpsilon parameter for adam optimizer. Only valid for adam. e is default. \nBelow is a sample Optimizer section. \nDefine data set. training validation test data set. \nStart with DataSet \nbuffersize unsigned int \nDefine Buffer size. usually it is greater than batch size. \nData buffer thread keeps reading the data from the file and stores the data into the data buffer. Meanwhile main thread gets the training data from this data buffer and feeds it to the model. This keyword defines the size of Data Buffer. \ntraindata string \ntraining data file path. The data must be saved as following \nfeature data i label data i feature data i label data i \nvaliddata string \nvalidation data file path. The data must be saved as following \nfeature data i label data i feature data i label data i \ntestdata string \ntest data file path. The data must be saved as following \nfeature data i label data i feature data i label data i \nlabeldata string \nlabel data file path. The data must be saved as following \nclass Name i class name i \nDescribe hyper parameters for layer. Order of layers in the model follows the order of definition of layers here from top to bottom. \nStart with layer name This layer name must be unique throughout network model. \ntype string \nType of Layer \nkernel_size unsigned int unsigned int \nKernel size for convolution layer \nbias_init_zero bool \ntoken to initialize bias with zeros. Setting to False would initialize bias randomly. \nnormalization bool \nnormalization on the input of this layer. \nstandardization bool \nstandardization on the input of this layer. \ninput_shape unsigned int unsigned int unsigned int \nshape of input shouldn t be zero \nactivation string \nset activation layer \nweight_regularizer string \nset weight decay \nweight_regularizer_constant float \ncoefficient for weight decay \nunit unsigned int \nset the output layer for fully connected layer \nweight_initializer string \nset weight initialization method \nfilters unsigned int \nset filters size for convolution layer \nstride unsigned int unsigned int \nset stride for convolution and pooling layer \npadding unsigned int unsigned int \nset padding for convolution and pooling layer \npool_size unsigned int unsigned int \nset pooling size for pooling layer \npooling string \ndefine type of pooling \nflatten bool \nflattens the output of this layer. \nEnabling this option is equivalent to attaching a flatten layer after the current layer. \nepsilon float \nEpsilon parameter for batch normalization layer. Default is \nEach layer requires different properties. \nLayer Properties conv2d \nBelow is sample for layers to define a model. \nThis allows to describe another model termed as backbone to be used in the model described by the current ini file. The backbone to be used can be described with another ini configuration file path or with model file for external frameworks. Support for backbones of external framework for Tensorflow Lite is provided natively with Tensorflow Lite framework. Support for backbones of other external frameworks is done using nnstreamer and its plugin. When using nnstreamer for external framework ensure to add the corresponding baseline ML framework and its corresponding nnstreamer plugin as a dependency or install manually. For example when using PyTorch based model as a backbone both the packages PyTorch and nnstreamer pytorch must be installed. \nBackbones made of nntrainer models described using ini support training the backbone also. However this is not supported with external frameworks. It is possible to describe a backbone inside a backbone ini configuration file as well as listing down multiple backbones to build a single model. For backbone ini configuration file Model and Dataset sections are ignored. \nDescribing a backbone is very similar to describing a layer. Start with a layer name which must be unique throughtout the model. In case of backbone the name of the backbone is prepended to the name of all the layers inside the backbone. \nbackbone string \nPath of the backbone file. Supported model files \ntrainable bool \nIf this backbone must be trained defaults to false Only supported for ini backbones nntrainer models \nPreload bool \nLoad pretrained weights from the saved modelfile of backbone defaults to false Only supported for ini backbone nntrainer models \nInputShape string \nSet the shape of the input layer for the backbone model. Only supported for ini backbones nntrainer models \nInputLayer string \nChoose the start layer for the backbone. This allows taking a subgraph starting with the specified layer name as a backbone. Only supported for ini backbones nntrainer models \nOutputLayer string \nChoose the end layer for the backbone. This allows taking a subgraph ending with the specified layer name as a backbone. Only supported for ini backbones nntrainer models Below is sample backbone section. \nThis has one input layer two convolution layers two pooling layers one flatten layer and one fully connected layer to classify MNIST example. \nIt takes x x gray data as an input. Adam optimizer is used to apply gradient and learning rate is e \n"});