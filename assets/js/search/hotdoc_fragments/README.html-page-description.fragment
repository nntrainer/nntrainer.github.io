fragment_downloaded_cb({"url":"README.html#page-description","fragment":"\n\n\n\nNNtrainer is a Software Framework for training Neural Network models on devices.\nNNtrainer is an Open Source Project. The aim of the NNtrainer is to develop a Software Framework to train neural network models on embedded devices which have relatively limited resources. Rather than training whole layers of a network, NNtrainer trains only one or a few layers of the layers added after a feature extractor.\nEven though NNTrainer can be used to train sub-models, it requires implementation of additional functionalities to train models obtained from other machine learning and deep learning libraries. In the current version, various machine learning algorithms such as k-Nearest Neighbor (k-NN), Neural Networks, Logistic Regression and Reinforcement Learning algorithms are implemented. We also provide examples for various tasks such as transfer learning of models. In some of these examples, deep learning models such as Mobilenet V2 trained with Tensorflow-lite, are used as feature extractors. All of these were tested on Galaxy S8 with Android and PC (Ubuntu 16.04).\nThis component defines layers which consist of a neural network model. Layers have their own properties to be set.\n| Keyword | Layer Name | Description |\n|:-------:|:---:|:---|\n|  conv2d | Convolution 2D |Convolution 2-Dimentional Layer |\n|  pooling2d | Pooling 2D |Pooling 2-Dimentional Layer. Support average / max / global average / global max pooing |\n| flatten | Flatten | Flatten Layer |\n| fully_connected | Fully Connected | Fully Connected Layer |\n| input | Input | Input Layer.  This is not always requied. |\n| batch_normalization | Batch Normalization Layer | Batch Normalization Layer. |\n| loss layer | loss layer | hidden from users |\n| activation | activaiton layer | set by layer property |\nNNTrainer Provides\n| Keyword | Optimizer Name | Description |\n|:-------:|:---:|:---:|\n| sgd | Stochastic Gradient Decent | - |\n| adam | Adaptive Moment Estimation | - |\nNNTrainer provides\n| Keyword | Loss Name | Description |\n|:-------:|:---:|:---:|\n| mse | Mean squared Error | - |\n| cross | Cross Entropy - sigmoid | if activation last layer is sigmoid |\n| cross | Cross Entropy - softmax | if activation last layer is softmax |\nNNTrainer provides\n| Keyword | Loss Name | Description |\n|:-------:|:---:|:---|\n| tanh | tanh function | set as layer property |\n| sigmoid | sigmoid function | set as layer property |\n| relu | relu function | set as layer propery |\n| softmax | softmax function | set as layer propery |\n| weight_initializer | Weight Initialization | Xavier(Normal/Uniform), LeCun(Normal/Uniform),  HE(Normal/Unifor) |\n| weight_regularizer | weight decay ( L2Norm only ) | needs set weight_regularizer_param & type |\n| learnig_rate_decay | learning rate decay | need to set step |\nTensor is responsible for calculation of a layer. It executes several operations such as addition, division, multiplication, dot production, data averaging and so on. In order to accelerate  calculation speed, CBLAS (C-Basic Linear Algebra: CPU) and CUBLAS (CUDA: Basic Linear Algebra) for PC (Especially NVIDIA GPU) are implemented for some of the operations. Later, these calculations will be optimized.\nCurrently, we supports lazy calculation mode to reduce complexity for copying tensors during calculations.\n| Keyword | Description |\n|:-------:|:---:|\n| 4D Tensor | B, C, H, W|\n| Add/sub/mul/div | - |\n| sum, average, argmax | - |\n| Dot, Transpose | - |\n| normalization, standardization | - |\n| save, read | - |\nNNTrainer provides\n| Keyword | Loss Name | Description |\n|:-------:|:---:|:---|\n| weight_initializer | Weight Initialization | Xavier(Normal/Uniform), LeCun(Normal/Uniform),  HE(Normal/Unifor) |\n| weight_regularizer | weight decay ( L2Norm only ) | needs set weight_regularizer_constant & type |\n| learnig_rate_decay | learning rate decay | need to set step |\nCurrently, we provide C APIs for Tizen. C++ API will be also provided soon.\nA demo application which enable user defined custom shortcut on galaxy watch.\nAn example to train mnist dataset. It consists two convolution 2d layer, 2 pooling 2d layer, flatten layer and fully connected layer.\nA reinforcement learning example with cartpole game. It is using DeepQ algorithm.\nTransfer learning examples with for image classification using the Cifar 10 dataset and for OCR. TFlite is used for feature extractor and modify last layer (fully connected layer) of network.\nAn example to demonstrate c api for Tizen. It is same transfer learing but written with tizen c api.~\nDeleted instead moved to a test\nA transfer learning example with for image classification using the Cifar 10 dataset. TFlite is used for feature extractor and compared with KNN.\nA logistic regression example using NNTrainer.\nInstructions for installing NNTrainer.\nInstructions for preparing NNTrainer for execution\nThe nntrainer is an open source project released under the terms of the Apache License version 2.0.\nContributions are welcome! Please see our Contributing Guide for more details.\n\n\nJijoong Moon\nMyungJoo Ham\nGeunsik Lim\n\n\nSangjung Woo\nWook Song\nJaeyun Jung\nHyoungjoo Ahn\nParichay Kapoor\nDongju Chae\nGichan Jang\nYongjoo Ahn\nJihoon Lee\nHyeonseok Lee\nMete Ozay\n\n"});